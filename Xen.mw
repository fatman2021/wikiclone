This guide describes how to start using Xen on your Gentoo system

== Introduction ==

The [http://www.xen.org/ Xen] technology allows you to run multiple operating systems on a single physical system, govern resource consumption and even migrate domains (which are the virtual environments in which a guest operating system runs) from one Xen-powered system to another. Xen requires the host operating system to support Xen (which, in this case, will be a Linux kernel) but guest operating systems can run unmodified ''if'' your hardware supports Intel Virtualization Technology (VT-x) or AMD Virtualization Technology (SVM). Otherwise your guest operating systems must also support Xen. 

This guide will talk you through the configuration steps necessary to get Xen up and running on Gentoo Linux. We will not discuss Xen itself (the Xen project has [http://xen.org/support/documentation.html decent documentation] available) nor will we talk about specialized setups that might be very interesting for Xen setups but are not Xen-related (like exporting Portage through NFS, booting Linux using PXE, etc.) 

== Preparing Domain0 (dom0) ==

=== Introduction ===

''Domain0'' is the primary domain under Xen, hosting the host operating system which governs all other domains. In this chapter we will prepare an existing Gentoo installation to become the host operating system in this domain and build the Xen-powered kernel so that Gentoo is ready to host other Xen domains. 

=== Rebuilding the Gentoo Installation? ===

A dramatic change that might be necessary is to rebuild the entire Gentoo installation with a different <code>CFLAGS</code> setting. Guest operating systems running under Xen might otherwise see major performance degradation. If you, however, are planning on checking out Xen rather than installing it for production use and are not terribly fond of rebuilding all programs, you can skip this step. In this case you will notice performance degradation but you will still be able to use Xen. 

{{Important|It is advised that, if you change your <code>CFLAGS</code> and build your system with a gcc lower than version 4, you do not have <code>-Os</code> set as it has been reported to produce broken code.}}

Add -mno-tls-direct-seg-refs ONLY if you have a 32-bit dom0. You don't need this flag if you have a 64-bit dom0.

{{FileBox|filename=/etc/portage/make.conf|title=CFLAGS change for mno-tls-direct-seg-refs|lang=bash|1=
CFLAGS="-O2 -march=pentium4 -pipe -mno-tls-direct-seg-refs"
}}

{{RootCmd|emerge -e world}}

If you boot your system using an initial ramdisk (initrd) you need to rebuild the initrd as well (which is best done by running all steps you would do when you rebuild your kernel). 

=== Installing Xen ===

Xen actually contains many components, so you'll need to install a few packages. 

{{RootCmd|emerge xen xen-tools gentoo-sources}}

=== Building the Kernel ===

Next we'll build the Linux kernel with Xen support. This kernel, whose sources are available at {{Path|/usr/src/linux}} , will be our main running kernel (i.e. the one running domain 0). In the <code>XEN</code> section you'll find drivers for all kinds of input/output, each driver having a ''backend'' and ''frontend'' implementation available. For the domain 0 kernel you need to select the ''backend'' implementation: these are used by the other domains (who use the ''frontend'' drivers) to communicate directly with the hardware. However, you should be able to configure the kernel to provide support for both frontend (guest) and backend (host) drivers. 

If you're wondering about networking: each interface in a domain has a point-to-point link to an interface on domain 0 (called {{Path|vifX.Y}} where X is the domain number and Y the Yth interface of that domain), so you can configure your network the way you want (bridging, NAT, etc.) 

{{KernelBox|title=Enabling Xen Support|
Processor type and features  --->
    [*] Linux guest support  --->
        [*]   Enable paravirtualization code
        [*]     Xen guest support
}}

{{KernelBox|title=Kernel Config|
Bus options (PCI etc.)  --->
    [*] Xen PCI Frontend
  
[*] Networking support --->
    Networking options  --->
        <*> 802.1d Ethernet Bridging
	[*] Network packet filtering framework (Netfilter) --->
	    [*] Advanced netfilter configuration
	        [*] Bridged IP/ARP packets filtering
  
Device Drivers --->
    [*] Block devices (NEW) --->
        <*> Xen block-device backend driver
    [*] Network device support --->
        <*> Xen backend network device
    Xen driver support --->
        [*] Xen memory balloon driver (NEW)
	[*]   Scrub pages before returning them to system (NEW)
	<*> Xen /dev/xen/evtchn device (NEW)
	[*] Backend driver support (NEW)
	<*> Xen filesystem (NEW)
	[*]   Create compatibility mount point /proc/xen (NEW)
	[*] Create xen entries under /sys/hypervisor (NEW)
	<*> userspace grant access device driver (NEW)
	<M> user-space grant reference allocator driver (NEW)
	<M> xen platform pci device driver (NEW)
}}

The shown kernel configuration should allow the kernel image to boot both as a host as well as a guest. However, if you want to, you can slim down the guest image kernel considerably. Refer to the Xen documentation for more information.

Once the kernel is built you'll find the kernel image immediately in the build directory (not inside {{Path|arch/}} or any other directory) called {{Path|vmlinuz}} . Copy it to {{Path|/boot}} and then configure your bootloader to use the Xen hypervisor (one of the components installed previously) which is stored as {{Path|/boot/xen.gz}} . In the bootloader configuration, add your newly built kernel as the kernel that Xen should boot. For instance, for GRUB: 

{{FileBox|filename=/boot/grub/grub.conf|title=GRUB Configuration for Xen|1=
title Xen Gentoo Linux 3.5
root (hd0,0)
kernel /boot/xen.gz
module /boot/kernel-3.5.x.y-xen0 root=/dev/sda3
}}

If you are using grub2, which provides auto-configuration scripts through grub2-mkconfig, you can also copy your kernel .config as config-<suffix> eg. config-3.5.x.y-xen0 in the above example. The scripts will automatically look for the Xen Dom0 options in kernel config and append Xen hypervisor boot lines to the grub menu.

Alternatively, the following command should do the trick:

{{RootCmd|genkernel --oldconfig --menuconfig --install --bootloader{{=}}grub --symlink --disklabel --lvm --mdadm --makeopts{{=}}-j9 all}}

Now reboot your system into Xen and check if you can do whatever you normally do on your system. If this is the case, you can edit your bootloader configuration to always boot into Xen. 

{{Note|If you wish to start guest domains automatically on boot add <code>xendomains</code> to the default runlevel as well and create a symlink in {{Path|/etc/xen/auto/}} to the Xen configuration files for the domains you wish to start.}}

== Creating an Unpriviledged Domain (domU) ==

=== Building the Kernel ===

Go to the Xen-powered Linux kernel source and, if necessary, update the configuration. It is wise to keep as many topics as possible similar to the main kernel. Then build the kernel and place the resulting {{Path|vmlinuz}} file where you want (we assume this is {{Path|/mnt/data/xen/kernel}} ): 

{{RootCmd|make O{{=}}~/build/domU
|cp ~/build/domU/vmlinuz /mnt/data/xen/kernel/kernel-3.5.x.y-xen}}

{{Note|On modern systems (e.g. xen-4.3.2, kernel-3.12) it is possible to use only one kernel for both: dom0 and domU. Just configure all options needed for a guest system in the main kernel. Don't forget to copy the modules {{Path|/lib/modules/<your kernel>}} to the guest system. }}

{{KernelBox|title=Additional kernel config options for domU|
Device Drivers --->
    [*] Block devices (NEW) --->
        <*>   Xen virtual block device support
    [*] Network device support --->
        <*>   Xen network device frontend driver
}}

=== Creating the Domain Disks ===

For best performance, it is best to dedicate a partition (or logical volume) to a domain rather than a file based filesystem. However, if you are going to use Xen primarily for tests using a file based filesystem does have its advantages (especially regarding maintenance). 

You can create a file based filesystem using <code>dd</code> and <code>mke2fs</code> (or any other file system creation tool). For instance, to create a 4 Gbyte ext4 filesystem: 

{{RootCmd|dd if{{=}}/dev/zero of{{=}}/mnt/data/xen/disks/ext4root.img bs{{=}}1M count{{=}}4096
|mkfs.ext4 /mnt/data/xen/disks/ext4root.img}}

=== Configuring a Domain ===

Next we create a Xen configuration file for a domain. You can store these configuration files where you want, for instance at {{Path|/mnt/data/xen/configs}} . As an example, we create a configuration file for a small Gentoo environment which uses the disk image we created previously: 

{{FileBox|filename=/mnt/data/xen/configs/gentoo|lang=python|1=
kernel = "/mnt/data/xen/kernel/kernel-3.5.x.y-xen"
memory = 512
name   = "gentoo"
(Map the disk image to the virtual /dev/sda1)
disk   = ['file:/mnt/data/xen/disks/ext4root.img,sda1,w']
root   = "/dev/sda1 ro"
}}

If you are using a block device (such as an lvm volume or partition) for the disk use 'phy:' instead of 'file:' and leave off /dev. For example: 

{{CodeBox|title=Using a block device|lang=python|1=
(LVM Volume)
disk = [ 'phy:lvm/xen-guest-root,sda1,w' ]
  
(Physical Partition)
disk = [ 'phy:sdb6,sda1,w' ]
}}

You can find example configuration files in {{Path|/etc/xen}}, which is also the default location for domU config files.

=== Launching the New Domain ===

Now we're all set and we can launch the new domain. If the disk image contained an operating system, we could just create and attach the domain using the <code>xl</code> command: 

{{RootCmd|xl create /etc/xen/gentoo -c}}

The domain would be booted inside the terminal in which you executed the command. However, in our case, the disk image is empty so the domain won't boot up in anything useful. To fix this, you can loop-mount the image and install Gentoo as you're used to. 

If you want to disconnect from the domain, press {{Path|Ctrl+]}} . You can always reconnect to the domains' console using <code>xl console gentoo</code> . However, there is only one console per domain, so only use it when you can't access the domain otherwise (for instance, through SSH).

If you are missing login prompt on the console, make sure you have entries like this in your inittab files on dom0 and domU pointing to {{Path|/dev/hvc0}}:

{{FileBox|filename=dom0:/etc/inittab|1=
c0:2345:respawn:/sbin/agetty 38400 hvc0 linux
}}

{{FileBox|filename=domU:/etc/inittab|1=
c0:2345:respawn:/sbin/agetty 38400 hvc0 linux
}}

To get the changes in {{Path|/etc/inittab}} to work, use:
{{RootCmd|telinit q}}

If it still not works, check the kernel config for the entries: {{Path|DEVTMPFS}} and {{Path|DEVTMPFS_MOUNT}}. They should be set.

{{KernelBox|title=Kernel Config DEVTMPFS|
Device Drivers --->
    Generic Driver Options  --->
        -*- Maintain a devtmpfs filesystem to mount at /dev
        [*]   Automount devtmpfs at /dev, after the kernel mounted the rootfs
}}

== Networking on Domains ==

=== Introduction ===

Xen works best when using a bridged mode network configuration. This means that your default network interface on the administrative domain becomes a bridge which accepts connections to the virtual domains as well as to the IP address your administrative domain has. 

=== Bridged Interfaces ===

Create a bridge interface by creating a new link to the networking init script as provided by Gentoo: 

{{RootCmd|cd /etc/init.d
|ln -s net.lo net.br0}}

Next, edit {{Path|/etc/conf.d/net}} and setup the bridge: 

{{FileBox|filename=dom0:/etc/conf.d/net|lang=bash|1=
# eth0 should NOT have an ip configured
config_eth0="null"

# configure bridge to replace eth0 on dom0. Make sure the netmask for the bridge includes ip addresses of all your domU's!
bridge_br0="eth0"
config_br0="192.168.XX.XX netmask 255.255.0.0 brd 192.168.255.255"
routes_br0="default via 192.168.XX.XX"
mac_br0="00:16:3e:5b:XX:XX"

# bridge options to make interface coming up immediately
brctl_br0="stp off
        setfd 0
        sethello 10"

rc_net_br0_need="net.eth0"
rc_net_br0_provide="!net"
}}

{{FileBox|filename=domU:/etc/conf.d/net|lang=bash|1=
config_eth0="192.168.1.200 netmask 255.255.255.0 brd 192.168.1.255"
# route all trafic through dom0 bridge address
routes_eth0="default via 192.168.XX.XX"
# make sure all your domU's have different mac addresses! Set them if needed in xen domU 
# config files in /etc/xen/<domU_name> with "vif = [ "ip=192.68.XX.XX,mac=XX:XX:XX:XX:XX:XX,bridge=br0" ];" !
}}

Finally, install the {{Package|net-misc/bridge-utils}} package, and make sure the {{Path|net.br0}} init script is loaded at boot. 

{{RootCmd|emerge net-misc/bridge-utils
|rc-update add net.br0 default}}

If you use bridged networks with real internet IP's in hosted environments, it may be necessary to add one or all of the following lines (depending on your environment) in your {{Path|/etc/sysctl.conf}} file to prevent redirects, that can cause intermittent network interruptions:

{{FileBox|filename=/etc/sysctl.conf|lang=ini|1=
net.ipv4.conf.all.send_redirects=0
net.ipv4.conf.eth0.send_redirects=0
net.ipv4.conf.br0.send_redirects=0
net.ipv4.conf.default.send_redirects=0
}}

To get the changes in {{Path|/etc/sysctl.conf}} to work, use:
{{RootCmd|sysctl -p /etc/sysctl.conf}}

if you encounter a poor network performance or if your domU network permanently stops working under heavy load (backup jobs, etc) (from outside it looks, like the instance would crash, but deactivating and activating the interface e.g. form the <code>xl console <domU name></code> with <code>/etc/init.d/net.eth0 stop/start</code>, restores normal operation) , use <code>ethtool</code> to improve/prevent it on all interfaces connected to the bridge (don't forget the bridge himself):

{{RootCmd|ethtool --offload <network device> gso off tso off sg off gro off}}

{{Note|You have to do it after each reboot, so use e.g. {{Path|/etc/crontab}} to make it permanent.}}

== Further Resources ==

=== Xen Documentation ===

*  [http://www.xen.org/support/documentation.html Official Xen documentation] 

*  [http://wiki.xen.org/ Xen Wiki] 

=== Xen Tools ===

*  [http://virt-manager.org/ app-emulation/virt-manager] is a graphical tool for administering virtual machines

=== Xen Tuning ===

*  [http://cloudnull.io/2012/07/xenserver-network-tuning Xen network tuning]

[[Category:Hypervisor]] {{Migrated|originalauthors=Sven Vermeulen, nightmorph}}
